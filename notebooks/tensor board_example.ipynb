{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the \n",
    "#execution of your graph, and show additional data like images that pass through it.\n",
    "#tensorboard readme https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/tensorboard/README.md\n",
    "\n",
    "#TensorBoard operates by reading TensorFlow events files, which contain summary data that you can \n",
    "#generate when running TensorFlow\n",
    "\n",
    "#Collect these by attaching tf.summary.scalar ops to the nodes that output the learning rate and loss \n",
    "#respectively. Then, give each scalar_summary a meaningful tag, like 'learning rate' or 'loss function'.\n",
    "\n",
    "#visualize distributions of activations coming off a particular layer, or the distribution of gradients or weights. \n",
    "#Collect this data by attaching tf.summary.histogram ops to the gradient outputs and to the variable that holds your \n",
    "#weights, respectively.\n",
    "\n",
    "#summary operations - https://www.tensorflow.org/api_guides/python/summary\n",
    "#these take the form of additional nodes\n",
    "#tf.summary.merge_all to combine them into a single op that generates all the summary data\n",
    "#Finally, to write this summary data to disk, pass the summary protobuf to a tf.summary.FileWriter.\n",
    "#The FileWriter takes a logdir in its constructor - this logdir is quite important, it's the directory \n",
    "#where all of the events will be written out. Also, the FileWriter can optionally take a Graph in its \n",
    "#constructor. If it receives a Graph object, then TensorBoard will visualize your graph along with tensor \n",
    "#shape information\n",
    "#Summary ops are ops, like tf.matmul or tf.nn.relu, which means they take in tensors, produce tensors, and are evaluated from within a TensorFlow graph. However, summary ops have a twist: the Tensors they produce contain serialized protobufs, which are written to disk and sent to TensorBoard\n",
    "\n",
    "# launch tensorboard --logdir=/tmp/mnist_logs\n",
    "#To run TensorBoard, use the following command (alternatively python -m tensorflow.tensorboard)\n",
    "#tensorboard --logdir=path/to/log-directory\n",
    "#Once TensorBoard is running, navigate your web browser to localhost:6006 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curr_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ad839f538ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#0.01 is learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#feed optimizer loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'curr_loss' is not defined"
     ]
    }
   ],
   "source": [
    "#what does the same code look like with Tensor Board\n",
    "import time\n",
    "sess = tf.Session()\n",
    "log_dir = \"./tesnorBoardFiles/NSC211_BKlecture_code\" #from currdir of notebook\n",
    "start_time = time.time() #record start time later \n",
    "\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32) #W\n",
    "#tf.summary.scalar('mean', mean)   \n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "summary = tf.summary.scalar(\"predicted\", linear_model)\n",
    "\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "summary = tf.summary.scalar(\"loss\", curr_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "#summary = tf.summary.merge_all()\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph. after sess created?\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        feed_dict = {x_: XOR_X, y_: XOR_Y}\n",
    "        sess.run(train, feed_dict)\n",
    "        curr_loss = sess.run(loss, feed_dict)\n",
    "        curr_predict = sess.run(loss, feed_dict)\n",
    "        #currsumm = sess.run(summary,feed_dict)\n",
    "        #print(\"loss:%s\\n\"%(curr_loss))\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        duration = time.time() - start_time\n",
    "        if i % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            #loss_array = curr_loss.eval(session=sess.run)\n",
    "            print(\"Step %d: loss = %.2f (%.3f sec)\" % (i, curr_loss, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary) #, feed_dict\n",
    "            summary_writer.add_summary(summary_str, i)\n",
    "            summary_writer.flush()\n",
    "\n",
    "\n",
    "#stake a look at the results\n",
    "predictions = sess.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#what does the same code look like with Tensor Board\n",
    "import time\n",
    "\n",
    "log_dir = \"./tesnorBoardFiles/NSC211_BKlecture_code\" #from currdir of notebook\n",
    "start_time = time.time() #record start time later \n",
    "\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "#tf.summary.scalar(\"predicted\", linear_model)\n",
    "\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "tf.summary.scalar(\"curr_loss\", loss)\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph. after sess created?\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        fdict = {x_: XOR_X, y_: XOR_Y}\n",
    "        _, curr_loss = sess.run([train, loss], fdict)\n",
    "        #curr_loss = sess.run(loss, feed_dict)\n",
    "        #curr_merged_summary,_ = sess.run([summary,train], fdict)\n",
    "        #currsumm = sess.run(summary,feed_dict)\n",
    "        #print(\"loss:%s\\n\"%(curr_loss))\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        duration = time.time() - start_time\n",
    "        if i % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            #loss_array = curr_loss.eval(session=sess.run)\n",
    "            print(\"Step %d: loss = %.2f (%.3f sec)\" % (i, curr_loss, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, fdict) #, feed_dict\n",
    "            summary_writer.add_summary(summary_str, i)\n",
    "            summary_writer.flush()\n",
    "\n",
    "\n",
    "#stake a look at the results\n",
    "predictions = sess.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datasci]",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
