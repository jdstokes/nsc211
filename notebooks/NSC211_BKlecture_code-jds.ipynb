{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Tutorial based on 6.1 Example: Learning XOR (page 170 in Deep Learning book) </b><p>\n",
    "The XOR function (“exclusive or”): operation on two binary values, x1 and x2.\n",
    "When only one of these values==1, the XOR function returns 1. Otherwise, 0.\n",
    "Right now, not concerned with statistical generalization. \n",
    "We want our network to perform correctly on the four points X = {[0,0] , [0,1] , [1,0] , and [1,1] }.<p>\n",
    "We can treat this problem as a regression problem and use a mean squared error loss function to simplify the math for this example as much as possible (there are better approaches for modeling binary data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 1 - Single Layer </b><p>We can minimize in closed form with respect to w and b using the normal equations.\n",
    "After solving the normal equations, we obtain w = 0 and b = 1/2. \n",
    "The linear model simply outputs 0.5 everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "##Network 1##\n",
    "sess1 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book solution (page 171)\n",
    "w = tf.Variable(tf.zeros([2,1]), tf.float32)\n",
    "b = tf.Variable([1/2.], tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess1.run(init)\n",
    "\n",
    "#operation node\n",
    "linear_model = tf.matmul(x_,w) + b \n",
    "\n",
    "#see what the predictions are\n",
    "print(sess1.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 2 - Two Layers </b><p>\n",
    "Solve the same problem using a model that learns a different feature space in which a linear model is able to represent the solution.\n",
    "Introduce a very simple feedforward network with one hidden layer containing two hidden units -> change what is given to output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "##Network 2##\n",
    "sess2 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book example (page 173)\n",
    "w1 = tf.Variable(tf.ones([2,2]), tf.float32) #W\n",
    "w2 = tf.Variable([[1.],[-2.]], tf.float32) #w\n",
    "b1 = tf.Variable([[0.,-1.]], tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros(1), tf.float32) #b\n",
    "\n",
    "init2 = tf.global_variables_initializer()\n",
    "sess2.run(init2)\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1, name=None) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#see what the predictions are\n",
    "print(sess2.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 3 - Two Layers + Optimization w/random initial param weights</b><p>\n",
    "In a real situation, there are lots of model parameters and training examples, \n",
    "we cannot guess the solution as we did above. Instead, a gradient-based optimization algorithm can \n",
    "find parameters that produce very little error. <p>\n",
    "Now solve the same problem but let's use gradient-based optimization to find params \n",
    "in order to do so need to measure error/loss (also need predicted values!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[  1.85531701e-06]\n",
      " [  9.99998808e-01]\n",
      " [  9.99998808e-01]\n",
      " [  5.28868668e-07]]\n",
      " hlayout:\n",
      " [[  0.00000000e+00   1.59907483e-08]\n",
      " [  0.00000000e+00   1.05575573e+00]\n",
      " [  0.00000000e+00   1.05575573e+00]\n",
      " [  9.38931525e-01   2.11151147e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 0.93893152  1.05575573]\n",
      " [ 0.93893152  1.05575573]] \n",
      "w2:\n",
      " [[-2.13007569]\n",
      " [ 0.94718593]] \n",
      "b1: [ -9.38931525e-01   1.59907483e-08] \n",
      "b2: [  1.84017085e-06] \n",
      "loss: 6.56407e-12\n"
     ]
    }
   ],
   "source": [
    "##Network 3##\n",
    "sess3 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#now we will define with some random values as starting points \n",
    "w1 = tf.Variable(tf.random_uniform([2,2], -2, 2), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 2), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init3 = tf.global_variables_initializer()\n",
    "sess3.run(init3)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess3.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#take a look at the results\n",
    "predictions = sess3.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess3.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess3.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 4 - Two Layers + Optimization w/non-random initial param weights </b><p>\n",
    "Using the approach above we will often find a different solution because the minima found depends \n",
    "on the rand. initial weights (if sess3 ran enough, will find similar solution 2 examples every once in a while).\n",
    "If we set the weights closer to the values provided in the example we consistently get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[  4.10483790e-06]\n",
      " [  9.99997258e-01]\n",
      " [  9.99997258e-01]\n",
      " [  1.46072352e-06]]\n",
      " hlayout:\n",
      " [[  2.11325517e-08   0.00000000e+00]\n",
      " [  9.82449114e-01   0.00000000e+00]\n",
      " [  9.82449114e-01   0.00000000e+00]\n",
      " [  1.96489823e+00   1.17923534e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 0.98244911  1.17923534]\n",
      " [ 0.98244911  1.17923534]] \n",
      "w2:\n",
      " [[ 1.01785743]\n",
      " [-1.69600487]] \n",
      "b1: [  2.11325517e-08  -1.17923534e+00] \n",
      "b2: [  4.08332789e-06] \n",
      "loss: 3.40185e-11\n"
     ]
    }
   ],
   "source": [
    "##Network 4##\n",
    "sess4 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init4 = tf.global_variables_initializer()\n",
    "sess4.run(init4)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess4.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#stake a look at the results\n",
    "predictions = sess4.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess4.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess4.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Is there a better way to save out diagnostic info? </b><p>\n",
    "The network below includes several summary ops and saves the graph so we can take a closer look in TensorBoard.\n",
    "Once the network has run and event files have been generated in your log directory enter the code below into a temrinal to launch TensorBoard: <p>\n",
    "tensorboard --logdir=/path/to/logdir <p>\n",
    "You should get a message that looks like this (paste the link in your internet browser):<p>\n",
    "Starting TensorBoard 41 on port 6006 \n",
    "(You can navigate to http://168.150.16.155:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 3.81 (0.417 sec)\n",
      "Step 100: loss = 0.98 (0.497 sec)\n",
      "Step 200: loss = 0.85 (0.564 sec)\n",
      "Step 300: loss = 0.47 (0.634 sec)\n",
      "Step 400: loss = 0.20 (0.714 sec)\n",
      "Step 500: loss = 0.07 (0.781 sec)\n",
      "Step 600: loss = 0.02 (0.853 sec)\n",
      "Step 700: loss = 0.01 (0.931 sec)\n",
      "Step 800: loss = 0.00 (0.994 sec)\n",
      "Step 900: loss = 0.00 (1.061 sec)\n",
      "Step 1000: loss = 0.00 (1.158 sec)\n",
      "Step 1100: loss = 0.00 (1.226 sec)\n",
      "Step 1200: loss = 0.00 (1.308 sec)\n",
      "Step 1300: loss = 0.00 (1.384 sec)\n",
      "Step 1400: loss = 0.00 (1.451 sec)\n",
      "Step 1500: loss = 0.00 (1.520 sec)\n",
      "Step 1600: loss = 0.00 (1.623 sec)\n",
      "Step 1700: loss = 0.00 (1.692 sec)\n",
      "Step 1800: loss = 0.00 (1.792 sec)\n",
      "Step 1900: loss = 0.00 (1.869 sec)\n",
      "Step 2000: loss = 0.00 (1.953 sec)\n",
      "Step 2100: loss = 0.00 (2.088 sec)\n",
      "Step 2200: loss = 0.00 (2.190 sec)\n",
      "Step 2300: loss = 0.00 (2.276 sec)\n",
      "Step 2400: loss = 0.00 (2.426 sec)\n",
      "Step 2500: loss = 0.00 (2.541 sec)\n",
      "Step 2600: loss = 0.00 (2.616 sec)\n",
      "Step 2700: loss = 0.00 (2.706 sec)\n",
      "Step 2800: loss = 0.00 (2.787 sec)\n",
      "Step 2900: loss = 0.00 (2.857 sec)\n",
      "Step 3000: loss = 0.00 (2.932 sec)\n",
      "Step 3100: loss = 0.00 (3.081 sec)\n",
      "Step 3200: loss = 0.00 (3.190 sec)\n",
      "Step 3300: loss = 0.00 (3.300 sec)\n",
      "Step 3400: loss = 0.00 (3.387 sec)\n",
      "Step 3500: loss = 0.00 (3.497 sec)\n",
      "Step 3600: loss = 0.00 (3.587 sec)\n",
      "Step 3700: loss = 0.00 (3.673 sec)\n",
      "Step 3800: loss = 0.00 (3.764 sec)\n",
      "Step 3900: loss = 0.00 (3.832 sec)\n",
      "Step 4000: loss = 0.00 (3.898 sec)\n",
      "Step 4100: loss = 0.00 (3.965 sec)\n",
      "Step 4200: loss = 0.00 (4.084 sec)\n",
      "Step 4300: loss = 0.00 (4.152 sec)\n",
      "Step 4400: loss = 0.00 (4.219 sec)\n",
      "Step 4500: loss = 0.00 (4.287 sec)\n",
      "Step 4600: loss = 0.00 (4.381 sec)\n",
      "Step 4700: loss = 0.00 (4.448 sec)\n",
      "Step 4800: loss = 0.00 (4.536 sec)\n",
      "Step 4900: loss = 0.00 (4.610 sec)\n",
      "Step 5000: loss = 0.00 (4.678 sec)\n",
      "Step 5100: loss = 0.00 (4.749 sec)\n",
      "Step 5200: loss = 0.00 (4.828 sec)\n",
      "Step 5300: loss = 0.00 (4.895 sec)\n",
      "Step 5400: loss = 0.00 (4.966 sec)\n",
      "Step 5500: loss = 0.00 (5.043 sec)\n",
      "Step 5600: loss = 0.00 (5.114 sec)\n",
      "Step 5700: loss = 0.00 (5.190 sec)\n",
      "Step 5800: loss = 0.00 (5.275 sec)\n",
      "Step 5900: loss = 0.00 (5.382 sec)\n",
      "Step 6000: loss = 0.00 (5.508 sec)\n",
      "Step 6100: loss = 0.00 (5.590 sec)\n",
      "Step 6200: loss = 0.00 (5.662 sec)\n",
      "Step 6300: loss = 0.00 (5.742 sec)\n",
      "Step 6400: loss = 0.00 (5.831 sec)\n",
      "Step 6500: loss = 0.00 (5.928 sec)\n",
      "Step 6600: loss = 0.00 (6.038 sec)\n",
      "Step 6700: loss = 0.00 (6.131 sec)\n",
      "Step 6800: loss = 0.00 (6.220 sec)\n",
      "Step 6900: loss = 0.00 (6.324 sec)\n",
      "Step 7000: loss = 0.00 (6.410 sec)\n",
      "Step 7100: loss = 0.00 (6.498 sec)\n",
      "Step 7200: loss = 0.00 (6.579 sec)\n",
      "Step 7300: loss = 0.00 (6.643 sec)\n",
      "Step 7400: loss = 0.00 (6.705 sec)\n",
      "Step 7500: loss = 0.00 (6.770 sec)\n",
      "Step 7600: loss = 0.00 (6.853 sec)\n",
      "Step 7700: loss = 0.00 (6.921 sec)\n",
      "Step 7800: loss = 0.00 (6.993 sec)\n",
      "Step 7900: loss = 0.00 (7.065 sec)\n",
      "Step 8000: loss = 0.00 (7.149 sec)\n",
      "Step 8100: loss = 0.00 (7.216 sec)\n",
      "Step 8200: loss = 0.00 (7.286 sec)\n",
      "Step 8300: loss = 0.00 (7.366 sec)\n",
      "Step 8400: loss = 0.00 (7.438 sec)\n",
      "Step 8500: loss = 0.00 (7.519 sec)\n",
      "Step 8600: loss = 0.00 (7.591 sec)\n",
      "Step 8700: loss = 0.00 (7.660 sec)\n",
      "Step 8800: loss = 0.00 (7.750 sec)\n",
      "Step 8900: loss = 0.00 (7.868 sec)\n",
      "Step 9000: loss = 0.00 (7.962 sec)\n",
      "Step 9100: loss = 0.00 (8.049 sec)\n",
      "Step 9200: loss = 0.00 (8.117 sec)\n",
      "Step 9300: loss = 0.00 (8.196 sec)\n",
      "Step 9400: loss = 0.00 (8.267 sec)\n",
      "Step 9500: loss = 0.00 (8.330 sec)\n",
      "Step 9600: loss = 0.00 (8.393 sec)\n",
      "Step 9700: loss = 0.00 (8.473 sec)\n",
      "Step 9800: loss = 0.00 (8.541 sec)\n",
      "Step 9900: loss = 0.00 (8.608 sec)\n",
      "predictions:\n",
      " [[  3.03041702e-06]\n",
      " [  9.99997675e-01]\n",
      " [  9.99997675e-01]\n",
      " [  1.69940904e-06]]\n",
      " hlayout:\n",
      " [[  0.00000000e+00   1.97172909e-08]\n",
      " [  0.00000000e+00   1.00057936e+00]\n",
      " [  0.00000000e+00   1.00057936e+00]\n",
      " [  1.22163975e+00   2.00115871e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 1.22163975  1.00057936]\n",
      " [ 1.22163975  1.00057936]] \n",
      "w2:\n",
      " [[-1.6371361 ]\n",
      " [ 0.99941564]] \n",
      "b1: [ -1.22163975e+00   1.97172909e-08] \n",
      "b2: [  3.01071123e-06] \n",
      "loss: 2.28788e-11\n"
     ]
    }
   ],
   "source": [
    "#Clear the default graph stack and reset the global default graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import time\n",
    "\n",
    "log_dir = \"tensorBoardFiles/NSC211_BKlecture_code\"#\"./tesnorBoardFiles/NSC211_BKlecture_code\" \n",
    "start_time = time.time() #record start time later \n",
    "\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32,name=\"L1_weights\") #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32,name=\"L2_weights\") #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32,name=\"L1_biases\") #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32,name=\"L2_biases\") #b\n",
    "\n",
    "#add summary histograms\n",
    "tf.summary.histogram('layer1_weights',w1)\n",
    "tf.summary.histogram('layer2_weights',w2)\n",
    "tf.summary.histogram('layer1_biases',b1)\n",
    "tf.summary.histogram('layer2_biases',b2)\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "tf.summary.histogram('transformed_output',transformedH)\n",
    "\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "tf.summary.histogram(\"predicted\", linear_model)\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "tf.summary.scalar(\"curr_loss\", loss)\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of summaries.\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        fdict = {x_: XOR_X, y_: XOR_Y}\n",
    "        #run the session and get loss and summary info\n",
    "        _, curr_loss, suminfo = sess.run([train, loss, summary], feed_dict=fdict)\n",
    "        duration = time.time() - start_time\n",
    "        # Write the summaries and print an overview every 100 trials.\n",
    "        if i % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print(\"Step %d: loss = %.2f (%.3f sec)\" % (i, curr_loss, duration))\n",
    "            # Update the events file.\n",
    "            summary_writer.add_summary(suminfo, i)\n",
    "            summary_writer.flush()\n",
    "\n",
    "#take a look at the results\n",
    "predictions = sess.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datasci]",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
