{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Tutorial based on 6.1 Example: Learning XOR (page 170 in Deep Learning book) </b><p>\n",
    "The XOR function (“exclusive or”): operation on two binary values, x1 and x2.\n",
    "When only one of these values==1, the XOR function returns 1. Otherwise, 0.\n",
    "Right now, not concerned with statistical generalization. \n",
    "We want our network to perform correctly on the four points X = {[0,0] , [0,1] , [1,0] , and [1,1] }.<p>\n",
    "We can treat this problem as a regression problem and use a mean squared error loss function to simplify the math for this example as much as possible (there are better approaches for modeling binary data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 1 - Single Layer </b><p>We can minimize in closed form with respect to w and b using the normal equations.\n",
    "After solving the normal equations, we obtain w = 0 and b = 1/2. \n",
    "The linear model simply outputs 0.5 everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "##Network 1##\n",
    "sess1 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book solution (page 171)\n",
    "w = tf.Variable(tf.zeros([2,1]), tf.float32)\n",
    "b = tf.Variable([1/2.], tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess1.run(init)\n",
    "\n",
    "#operation node\n",
    "linear_model = tf.matmul(x_,w) + b \n",
    "\n",
    "#see what the predictions are\n",
    "print(sess1.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 2 - Two Layers </b><p>\n",
    "Solve the same problem using a model that learns a different feature space in which a linear model is able to represent the solution.\n",
    "Introduce a very simple feedforward network with one hidden layer containing two hidden units -> change what is given to output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "##Network 2##\n",
    "sess2 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book example (page 173)\n",
    "w1 = tf.Variable(tf.ones([2,2]), tf.float32) #W\n",
    "w2 = tf.Variable([[1.],[-2.]], tf.float32) #w\n",
    "b1 = tf.Variable([[0.,-1.]], tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros(1), tf.float32) #b\n",
    "\n",
    "init2 = tf.global_variables_initializer()\n",
    "sess2.run(init2)\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1, name=None) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#see what the predictions are\n",
    "print(sess2.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 3 - Two Layers + Optimization w/random initial param weights</b><p>\n",
    "In a real situation, there are lots of model parameters and training examples, \n",
    "we cannot guess the solution as we did above. Instead, a gradient-based optimization algorithm can \n",
    "find parameters that produce very little error. <p>\n",
    "Now solve the same problem but let's use gradient-based optimization to find params \n",
    "in order to do so need to measure error/loss (also need predicted values!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[ 0.33333355]\n",
      " [ 1.        ]\n",
      " [ 0.33333355]\n",
      " [ 0.33333355]]\n",
      " hlayout:\n",
      " [[ 0.         0.       ]\n",
      " [ 0.         1.0122894]\n",
      " [ 0.         0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "w1:\n",
      " [[ 0.09127363 -1.01660681]\n",
      " [-0.91181755  1.01512766]] \n",
      "w2:\n",
      " [[-1.87455237]\n",
      " [ 0.65857297]] \n",
      "b1: [-0.13342872 -0.00283831] \n",
      "b2: [ 0.33333355] \n",
      "loss: 0.666667\n"
     ]
    }
   ],
   "source": [
    "##Network 3##\n",
    "sess3 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#now we will define with some random values as starting points \n",
    "w1 = tf.Variable(tf.random_uniform([2,2], -2, 2), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 2), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init3 = tf.global_variables_initializer()\n",
    "sess3.run(init3)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess3.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#take a look at the results\n",
    "predictions = sess3.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess3.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess3.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 4 - Two Layers + Optimization w/non-random initial param weights </b><p>\n",
    "Using the approach above we will often find a different solution because the minima found depends \n",
    "on the rand. initial weights (if sess3 ran enough, will find similar solution 2 examples every once in a while).\n",
    "If we set the weights closer to the values provided in the example we consistently get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[  2.80746485e-06]\n",
      " [  9.99997854e-01]\n",
      " [  9.99997854e-01]\n",
      " [  1.48406957e-06]]\n",
      " hlayout:\n",
      " [[  1.32329925e-08   0.00000000e+00]\n",
      " [  1.09424460e+00   0.00000000e+00]\n",
      " [  1.09424460e+00   0.00000000e+00]\n",
      " [  2.18848920e+00   1.14814603e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 1.0942446   1.14814603]\n",
      " [ 1.0942446   1.14814603]] \n",
      "w2:\n",
      " [[ 0.91386795]\n",
      " [-1.7419312 ]] \n",
      "b1: [  1.32329925e-08  -1.14814603e+00] \n",
      "b2: [  2.79537176e-06] \n",
      "loss: 1.9293e-11\n"
     ]
    }
   ],
   "source": [
    "##Network 4##\n",
    "sess4 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init4 = tf.global_variables_initializer()\n",
    "sess4.run(init4)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess4.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#stake a look at the results\n",
    "predictions = sess4.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess4.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess4.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Is there a better way to save out diagnostic info? </b><p>\n",
    "The network below includes several summary ops and saves the graph so we can take a closer look in TensorBoard.\n",
    "Once the network has run and event files have been generated in your log directory enter the code below into a temrinal to launch TensorBoard: <p>\n",
    "tensorboard --logdir=/path/to/logdir <p>\n",
    "You should get a message that looks like this (paste the link in your internet browser):<p>\n",
    "Starting TensorBoard 41 on port 6006 \n",
    "(You can navigate to http://168.150.16.155:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/Users/bmk",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ed3c7e23db3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Instantiate a SummaryWriter to output summaries and the Graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jdstokes/anaconda/envs/datasci/lib/python2.7/site-packages/tensorflow/python/summary/writer/writer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def)\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDEPRECATED\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUse\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0margument\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mevent_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush_secs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jdstokes/anaconda/envs/datasci/lib/python2.7/site-packages/tensorflow/python/summary/writer/event_file_writer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, max_queue, flush_secs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     self._ev_writer = pywrap_tensorflow.EventsWriter(\n",
      "\u001b[0;32m/Users/jdstokes/anaconda/envs/datasci/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jdstokes/anaconda/envs/datasci/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jdstokes/anaconda/envs/datasci/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m           \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /Users/bmk"
     ]
    }
   ],
   "source": [
    "#Clear the default graph stack and reset the global default graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import time\n",
    "\n",
    "log_dir = \"/Users/bmk/Google Drive/desktopBackup/PSC211_S17/tensorBoardFiles/NSC211_BKlecture_code\"#\"./tesnorBoardFiles/NSC211_BKlecture_code\" \n",
    "start_time = time.time() #record start time later \n",
    "\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32,name=\"L1_weights\") #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32,name=\"L2_weights\") #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32,name=\"L1_biases\") #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32,name=\"L2_biases\") #b\n",
    "\n",
    "#add summary histograms\n",
    "tf.summary.histogram('layer1_weights',w1)\n",
    "tf.summary.histogram('layer2_weights',w2)\n",
    "tf.summary.histogram('layer1_biases',b1)\n",
    "tf.summary.histogram('layer2_biases',b2)\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "tf.summary.histogram('transformed_output',transformedH)\n",
    "\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "tf.summary.histogram(\"predicted\", linear_model)\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "tf.summary.scalar(\"curr_loss\", loss)\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of summaries.\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        fdict = {x_: XOR_X, y_: XOR_Y}\n",
    "        #run the session and get loss and summary info\n",
    "        _, curr_loss, suminfo = sess.run([train, loss, summary], feed_dict=fdict)\n",
    "        duration = time.time() - start_time\n",
    "        # Write the summaries and print an overview every 100 trials.\n",
    "        if i % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print(\"Step %d: loss = %.2f (%.3f sec)\" % (i, curr_loss, duration))\n",
    "            # Update the events file.\n",
    "            summary_writer.add_summary(suminfo, i)\n",
    "            summary_writer.flush()\n",
    "\n",
    "#take a look at the results\n",
    "predictions = sess.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datasci]",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
