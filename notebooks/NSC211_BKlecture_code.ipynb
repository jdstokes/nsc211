{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Tutorial based on 6.1 Example: Learning XOR (page 170 in Deep Learning book) </b><p>\n",
    "The XOR function (“exclusive or”): operation on two binary values, x1 and x2.\n",
    "When only one of these values==1, the XOR function returns 1. Otherwise, 0.\n",
    "Right now, not concerned with statistical generalization. \n",
    "We want our network to perform correctly on the four points X = {[0,0] , [0,1] , [1,0] , and [1,1] }.<p>\n",
    "We can treat this problem as a regression problem and use a mean squared error loss function to simplify the math for this example as much as possible (there are better approaches for modeling binary data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 1 - Single Layer </b><p>We can minimize in closed form with respect to w and b using the normal equations.\n",
    "After solving the normal equations, we obtain w = 0 and b = 1/2. \n",
    "The linear model simply outputs 0.5 everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "##Network 1##\n",
    "sess1 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book solution (page 171)\n",
    "w = tf.Variable(tf.zeros([2,1]), tf.float32)\n",
    "b = tf.Variable([1/2.], tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess1.run(init)\n",
    "\n",
    "#operation node\n",
    "linear_model = tf.matmul(x_,w) + b \n",
    "\n",
    "#see what the predictions are\n",
    "print(sess1.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 2 - Two Layers </b><p>\n",
    "Solve the same problem using a model that learns a different feature space in which a linear model is able to represent the solution.\n",
    "Introduce a very simple feedforward network with one hidden layer containing two hidden units -> change what is given to output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "##Network 2##\n",
    "sess2 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book example (page 173)\n",
    "w1 = tf.Variable(tf.ones([2,2]), tf.float32) #W\n",
    "w2 = tf.Variable([[1.],[-2.]], tf.float32) #w\n",
    "b1 = tf.Variable([[0.,-1.]], tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros(1), tf.float32) #b\n",
    "\n",
    "init2 = tf.global_variables_initializer()\n",
    "sess2.run(init2)\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1, name=None) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#see what the predictions are\n",
    "print(sess2.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 3 - Two Layers + Optimization w/random initial param weights</b><p>\n",
    "In a real situation, there are lots of model parameters and training examples, \n",
    "we cannot guess the solution as we did above. Instead, a gradient-based optimization algorithm can \n",
    "find parameters that produce very little error. <p>\n",
    "Now solve the same problem but let's use gradient-based optimization to find params \n",
    "in order to do so need to measure error/loss (also need predicted values!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[  2.73580895e-06]\n",
      " [  9.99998331e-01]\n",
      " [  9.99998331e-01]\n",
      " [  5.58800139e-07]]\n",
      " hlayout:\n",
      " [[  2.82178867e-08   0.00000000e+00]\n",
      " [  9.03214455e-01   0.00000000e+00]\n",
      " [  9.03214455e-01   0.00000000e+00]\n",
      " [  1.80642891e+00   9.32806492e-01]]\n",
      "\n",
      "w1:\n",
      " [[ 0.90321445  0.93280649]\n",
      " [ 0.90321445  0.93280649]] \n",
      "w2:\n",
      " [[ 1.10715199]\n",
      " [-2.14406037]] \n",
      "b1: [  2.82178867e-08  -9.32806492e-01] \n",
      "b2: [  2.70456735e-06] \n",
      "loss: 1.33676e-11\n"
     ]
    }
   ],
   "source": [
    "##Network 3##\n",
    "sess3 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#now we will define with some random values as starting points \n",
    "w1 = tf.Variable(tf.random_uniform([2,2], -2, 2), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 2), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init3 = tf.global_variables_initializer()\n",
    "sess3.run(init3)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess3.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#take a look at the results\n",
    "predictions = sess3.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess3.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess3.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network 4 - Two Layers + Optimization w/non-random initial param weights </b><p>\n",
    "Using the approach above we will often find a different solution because the minima found depends \n",
    "on the rand. initial weights (if sess3 ran enough, will find similar solution 2 examples every once in a while).\n",
    "If we set the weights closer to the values provided in the example we consistently get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[  2.45183492e-06]\n",
      " [  9.99998093e-01]\n",
      " [  9.99998093e-01]\n",
      " [  1.36286235e-06]]\n",
      " hlayout:\n",
      " [[  1.96318073e-08   0.00000000e+00]\n",
      " [  1.22018981e+00   0.00000000e+00]\n",
      " [  1.22018981e+00   0.00000000e+00]\n",
      " [  2.44037962e+00   1.06932676e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 1.22018981  1.06932676]\n",
      " [ 1.22018981  1.06932676]] \n",
      "w2:\n",
      " [[ 0.81954104]\n",
      " [-1.87032855]] \n",
      "b1: [  1.96318073e-08  -1.06932676e+00] \n",
      "b2: [  2.43574596e-06] \n",
      "loss: 1.51448e-11\n"
     ]
    }
   ],
   "source": [
    "##Network 4##\n",
    "sess4 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init4 = tf.global_variables_initializer()\n",
    "sess4.run(init4)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess4.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#stake a look at the results\n",
    "predictions = sess4.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess4.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess4.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Is there a better way to save out diagnostic info? </b><p>\n",
    "The network below includes several summary ops and saves the graph so we can take a closer look in TensorBoard.\n",
    "Once the network has run and event files have been generated in your log directory enter the code below into a temrinal to launch TensorBoard: <p>\n",
    "tensorboard --logdir=/path/to/logdir <p>\n",
    "You should get a message that looks like this (paste the link in your internet browser):<p>\n",
    "Starting TensorBoard 41 on port 6006 \n",
    "(You can navigate to http://168.150.16.155:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 8.68 (0.351 sec)\n",
      "Step 100: loss = 0.15 (0.411 sec)\n",
      "Step 200: loss = 0.05 (0.473 sec)\n",
      "Step 300: loss = 0.01 (0.540 sec)\n",
      "Step 400: loss = 0.00 (0.610 sec)\n",
      "Step 500: loss = 0.00 (0.672 sec)\n",
      "Step 600: loss = 0.00 (0.735 sec)\n",
      "Step 700: loss = 0.00 (0.797 sec)\n",
      "Step 800: loss = 0.00 (0.868 sec)\n",
      "Step 900: loss = 0.00 (0.937 sec)\n",
      "Step 1000: loss = 0.00 (0.987 sec)\n",
      "Step 1100: loss = 0.00 (1.042 sec)\n",
      "Step 1200: loss = 0.00 (1.105 sec)\n",
      "Step 1300: loss = 0.00 (1.160 sec)\n",
      "Step 1400: loss = 0.00 (1.215 sec)\n",
      "Step 1500: loss = 0.00 (1.270 sec)\n",
      "Step 1600: loss = 0.00 (1.333 sec)\n",
      "Step 1700: loss = 0.00 (1.389 sec)\n",
      "Step 1800: loss = 0.00 (1.448 sec)\n",
      "Step 1900: loss = 0.00 (1.502 sec)\n",
      "Step 2000: loss = 0.00 (1.564 sec)\n",
      "Step 2100: loss = 0.00 (1.619 sec)\n",
      "Step 2200: loss = 0.00 (1.674 sec)\n",
      "Step 2300: loss = 0.00 (1.732 sec)\n",
      "Step 2400: loss = 0.00 (1.795 sec)\n",
      "Step 2500: loss = 0.00 (1.850 sec)\n",
      "Step 2600: loss = 0.00 (1.904 sec)\n",
      "Step 2700: loss = 0.00 (1.959 sec)\n",
      "Step 2800: loss = 0.00 (2.020 sec)\n",
      "Step 2900: loss = 0.00 (2.075 sec)\n",
      "Step 3000: loss = 0.00 (2.129 sec)\n",
      "Step 3100: loss = 0.00 (2.181 sec)\n",
      "Step 3200: loss = 0.00 (2.240 sec)\n",
      "Step 3300: loss = 0.00 (2.296 sec)\n",
      "Step 3400: loss = 0.00 (2.352 sec)\n",
      "Step 3500: loss = 0.00 (2.406 sec)\n",
      "Step 3600: loss = 0.00 (2.466 sec)\n",
      "Step 3700: loss = 0.00 (2.523 sec)\n",
      "Step 3800: loss = 0.00 (2.577 sec)\n",
      "Step 3900: loss = 0.00 (2.632 sec)\n",
      "Step 4000: loss = 0.00 (2.690 sec)\n",
      "Step 4100: loss = 0.00 (2.731 sec)\n",
      "Step 4200: loss = 0.00 (2.771 sec)\n",
      "Step 4300: loss = 0.00 (2.822 sec)\n",
      "Step 4400: loss = 0.00 (2.877 sec)\n",
      "Step 4500: loss = 0.00 (2.936 sec)\n",
      "Step 4600: loss = 0.00 (2.991 sec)\n",
      "Step 4700: loss = 0.00 (3.046 sec)\n",
      "Step 4800: loss = 0.00 (3.100 sec)\n",
      "Step 4900: loss = 0.00 (3.167 sec)\n",
      "Step 5000: loss = 0.00 (3.221 sec)\n",
      "Step 5100: loss = 0.00 (3.276 sec)\n",
      "Step 5200: loss = 0.00 (3.330 sec)\n",
      "Step 5300: loss = 0.00 (3.390 sec)\n",
      "Step 5400: loss = 0.00 (3.448 sec)\n",
      "Step 5500: loss = 0.00 (3.504 sec)\n",
      "Step 5600: loss = 0.00 (3.560 sec)\n",
      "Step 5700: loss = 0.00 (3.622 sec)\n",
      "Step 5800: loss = 0.00 (3.681 sec)\n",
      "Step 5900: loss = 0.00 (3.742 sec)\n",
      "Step 6000: loss = 0.00 (3.801 sec)\n",
      "Step 6100: loss = 0.00 (3.865 sec)\n",
      "Step 6200: loss = 0.00 (3.931 sec)\n",
      "Step 6300: loss = 0.00 (3.985 sec)\n",
      "Step 6400: loss = 0.00 (4.039 sec)\n",
      "Step 6500: loss = 0.00 (4.103 sec)\n",
      "Step 6600: loss = 0.00 (4.157 sec)\n",
      "Step 6700: loss = 0.00 (4.221 sec)\n",
      "Step 6800: loss = 0.00 (4.276 sec)\n",
      "Step 6900: loss = 0.00 (4.338 sec)\n",
      "Step 7000: loss = 0.00 (4.398 sec)\n",
      "Step 7100: loss = 0.00 (4.458 sec)\n",
      "Step 7200: loss = 0.00 (4.518 sec)\n",
      "Step 7300: loss = 0.00 (4.586 sec)\n",
      "Step 7400: loss = 0.00 (4.645 sec)\n",
      "Step 7500: loss = 0.00 (4.705 sec)\n",
      "Step 7600: loss = 0.00 (4.766 sec)\n",
      "Step 7700: loss = 0.00 (4.831 sec)\n",
      "Step 7800: loss = 0.00 (4.893 sec)\n",
      "Step 7900: loss = 0.00 (4.955 sec)\n",
      "Step 8000: loss = 0.00 (5.016 sec)\n",
      "Step 8100: loss = 0.00 (5.082 sec)\n",
      "Step 8200: loss = 0.00 (5.143 sec)\n",
      "Step 8300: loss = 0.00 (5.204 sec)\n",
      "Step 8400: loss = 0.00 (5.266 sec)\n",
      "Step 8500: loss = 0.00 (5.327 sec)\n",
      "Step 8600: loss = 0.00 (5.379 sec)\n",
      "Step 8700: loss = 0.00 (5.439 sec)\n",
      "Step 8800: loss = 0.00 (5.496 sec)\n",
      "Step 8900: loss = 0.00 (5.554 sec)\n",
      "Step 9000: loss = 0.00 (5.609 sec)\n",
      "Step 9100: loss = 0.00 (5.664 sec)\n",
      "Step 9200: loss = 0.00 (5.725 sec)\n",
      "Step 9300: loss = 0.00 (5.789 sec)\n",
      "Step 9400: loss = 0.00 (5.851 sec)\n",
      "Step 9500: loss = 0.00 (5.910 sec)\n",
      "Step 9600: loss = 0.00 (5.970 sec)\n",
      "Step 9700: loss = 0.00 (6.036 sec)\n",
      "Step 9800: loss = 0.00 (6.098 sec)\n",
      "Step 9900: loss = 0.00 (6.158 sec)\n",
      "predictions:\n",
      " [[  2.60086813e-06]\n",
      " [  9.99996305e-01]\n",
      " [  9.99999702e-01]\n",
      " [  1.40270299e-06]]\n",
      " hlayout:\n",
      " [[  0.00000000e+00   7.02981495e-09]\n",
      " [  0.00000000e+00   1.15771043e+00]\n",
      " [  1.91462040e-02   1.19908023e+00]\n",
      " [  1.09084094e+00   2.35679054e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 1.09084094  1.19908023]\n",
      " [ 1.07169461  1.15771043]] \n",
      "w2:\n",
      " [[-1.86619544]\n",
      " [ 0.8637684 ]] \n",
      "b1: [ -1.07169473e+00   7.02981495e-09] \n",
      "b2: [  2.59479589e-06] \n",
      "loss: 2.24775e-11\n"
     ]
    }
   ],
   "source": [
    "#Clear the default graph stack and reset the global default graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import time\n",
    "\n",
    "log_dir = \"/Users/bmk/Google Drive/desktopBackup/PSC211_S17/tensorBoardFiles/NSC211_BKlecture_code\"#\"./tesnorBoardFiles/NSC211_BKlecture_code\" \n",
    "start_time = time.time() #record start time later \n",
    "\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32,name=\"L1_weights\") #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32,name=\"L2_weights\") #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32,name=\"L1_biases\") #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32,name=\"L2_biases\") #b\n",
    "\n",
    "#add summary histograms\n",
    "tf.summary.histogram('layer1_weights',w1)\n",
    "tf.summary.histogram('layer2_weights',w2)\n",
    "tf.summary.histogram('layer1_biases',b1)\n",
    "tf.summary.histogram('layer2_biases',b2)\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "tf.summary.histogram('transformed_output',transformedH)\n",
    "\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "tf.summary.histogram(\"predicted\", linear_model)\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "tf.summary.scalar(\"curr_loss\", loss)\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "# Build the summary Tensor based on the TF collection of summaries.\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        fdict = {x_: XOR_X, y_: XOR_Y}\n",
    "        #run the session and get loss and summary info\n",
    "        _, curr_loss, suminfo = sess.run([train, loss, summary], feed_dict=fdict)\n",
    "        duration = time.time() - start_time\n",
    "        # Write the summaries and print an overview every 100 trials.\n",
    "        if i % 100 == 0:\n",
    "            # Print status to stdout.\n",
    "            print(\"Step %d: loss = %.2f (%.3f sec)\" % (i, curr_loss, duration))\n",
    "            # Update the events file.\n",
    "            summary_writer.add_summary(suminfo, i)\n",
    "            summary_writer.flush()\n",
    "\n",
    "#take a look at the results\n",
    "predictions = sess.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nsc211v2]",
   "language": "python",
   "name": "conda-env-nsc211v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
