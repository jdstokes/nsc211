{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tutorial based on 6.1 Example: Learning XOR (page 170 in Deep Learning book)\n",
    "#The XOR function (“exclusive or”): operation on two binary values, x1 and x2.\n",
    "#When only one of these values==1, the XOR function returns 1. Otherwise, 0.\n",
    "#Right now, not concerned with statistical generalization. \n",
    "#We want our network to perform correctly on the four points X = {[0,0] , [0,1] , [1,0] , and [1,1] }."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can treat this problem as a regression problem and use a mean squared error loss function. \n",
    "#We choose this loss function to simplify the math for this example as much as possible. \n",
    "#(there are other, more appropriate approaches for modeling binary data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "#We can minimize in closed form with respect to w and b using the normal equations.\n",
    "#After solving the normal equations, we obtain w = 0 and b = 1/2. \n",
    "#The linear model simply outputs 0.5 everywhere.\n",
    "sess = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book solution (page 171)\n",
    "w = tf.Variable(tf.zeros([2,1]), tf.float32)\n",
    "b = tf.Variable([1/2.], tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#operation node\n",
    "linear_model = tf.matmul(x_,w) + b \n",
    "\n",
    "#see what the predictions are\n",
    "print(sess.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "#solve problem using a model that learns a different feature space in which a \n",
    "#linear model is able to represent the solution.\n",
    "#introduce a very simple feedforward network with one hidden layer containing two hidden units.\n",
    "#change what is given to output layer\n",
    "sess2 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "\n",
    "#placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "#use weights/biases from book example (page 173)\n",
    "w1 = tf.Variable(tf.ones([2,2]), tf.float32) #W\n",
    "w2 = tf.Variable([[1.],[-2.]], tf.float32) #w\n",
    "b1 = tf.Variable([[0.,-1.]], tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros(1), tf.float32) #b\n",
    "\n",
    "init2 = tf.global_variables_initializer()\n",
    "sess2.run(init2)\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1, name=None) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#see what the predictions are\n",
    "print(sess2.run(linear_model, {x_: XOR_X})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[  1.80513598e-06]\n",
      " [  9.99997258e-01]\n",
      " [  9.99999046e-01]\n",
      " [  1.68359838e-06]]\n",
      " hlayout:\n",
      " [[  0.00000000e+00   1.09952327e-03]\n",
      " [  1.19209290e-07   7.78593838e-01]\n",
      " [  4.13178086e-01   1.47565293e+00]\n",
      " [  1.33489180e+00   2.25314736e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 1.33489168  1.47455347]\n",
      " [ 0.92171371  0.77749431]] \n",
      "w2:\n",
      " [[-2.16986394]\n",
      " [ 1.28617752]] \n",
      "b1: [-0.92171359  0.00109952] \n",
      "b2: [-0.00141238] \n",
      "loss: 1.45201e-11\n"
     ]
    }
   ],
   "source": [
    "#In a real situation, there are lots of model parameters and training examples, \n",
    "#we cannot guess the solution as we did above. Instead, a gradient-based optimization algorithm can \n",
    "#find parameters that produce very little error. \n",
    "#now solve the same problem but let's use gradient-based optimization to find params \n",
    "#in order to do so need to measure error/loss (also need predicted values!)\n",
    "sess3 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#now we will define with some random values as starting points \n",
    "w1 = tf.Variable(tf.random_uniform([2,2], -2, 2), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 2), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init3 = tf.global_variables_initializer()\n",
    "sess3.run(init3)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess3.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#take a look at the results\n",
    "predictions = sess3.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess3.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess3.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[  2.63455718e-06]\n",
      " [  9.99998033e-01]\n",
      " [  9.99997914e-01]\n",
      " [  1.41855867e-06]]\n",
      " hlayout:\n",
      " [[  2.79839547e-08   0.00000000e+00]\n",
      " [  1.17059112e+00   0.00000000e+00]\n",
      " [  1.17059100e+00   0.00000000e+00]\n",
      " [  2.34118223e+00   1.05052531e+00]]\n",
      "\n",
      "w1:\n",
      " [[ 1.170591    1.05052531]\n",
      " [ 1.17059112  1.05052531]] \n",
      "w2:\n",
      " [[ 0.85426533]\n",
      " [-1.9038018 ]] \n",
      "b1: [  2.79839547e-08  -1.05052531e+00] \n",
      "b2: [  2.61065156e-06] \n",
      "loss: 1.71742e-11\n"
     ]
    }
   ],
   "source": [
    "#using the approach above we will often find a different solution because the minima found depends \n",
    "#on the rand. initial weights (if sess3 ran enough, will find similar solution 2 examples every once in a while)\n",
    "#if we set the weights closer to the values provided in the example we consistently get the same results \n",
    "sess4 = tf.Session()\n",
    "#input data\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]] #input\n",
    "XOR_Y = [[0],[1],[1],[0]] #predicted\n",
    "\n",
    "#placeholders, now we need one for predicted vals too\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\") \n",
    "#constrain rand. values\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], .7, 1.3), tf.float32) #W\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -2, 1), tf.float32) #w\n",
    "b1 = tf.Variable(tf.zeros([2]), tf.float32) #c\n",
    "b2 = tf.Variable(tf.zeros([1]), tf.float32) #b\n",
    "\n",
    "#operation nodes\n",
    "transformedH = tf.nn.relu(tf.matmul(x_,w1) + b1) #hidden layer with rect. linear act. func.\n",
    "linear_model = tf.matmul(transformedH, w2) + b2\n",
    "\n",
    "#MSE\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y_)) #create error vector.We call tf.square to square that error.\n",
    "\n",
    "#gradient descent \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is learning rate\n",
    "train = optimizer.minimize(loss) #feed optimizer loss function \n",
    "\n",
    "init4 = tf.global_variables_initializer()\n",
    "sess4.run(init4)\n",
    "\n",
    "#train it\n",
    "for i in range(10000):\n",
    "        sess4.run(train, {x_: XOR_X, y_: XOR_Y})\n",
    "\n",
    "#stake a look at the results\n",
    "predictions = sess4.run(linear_model, {x_: XOR_X}) \n",
    "curr_w1, curr_w2, curr_b1, curr_b2, curr_loss  = sess4.run([w1, w2, b1, b2, loss], {x_: XOR_X, y_: XOR_Y})\n",
    "hidlay  = sess4.run(transformedH, {x_: XOR_X, y_: XOR_Y})\n",
    "print(\"predictions:\\n %s\\n hlayout:\\n %s\\n\"%(predictions,hidlay))\n",
    "print(\"w1:\\n %s \\nw2:\\n %s \\nb1: %s \\nb2: %s \\nloss: %s\"%(curr_w1, curr_w2, curr_b1, curr_b2, curr_loss))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nsc211v2]",
   "language": "python",
   "name": "conda-env-nsc211v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
